{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831013bb-7372-4e56-923b-a0880c87bb5f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Structured pdf\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame()\n",
    "with pdfplumber.open(r\"data_pdf_1.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        df = pd.concat((df, pd.DataFrame(page.extract_table())), ignore_index=True)\n",
    "\n",
    "df.columns = df.iloc[0]   \n",
    "df = df[1:].reset_index(drop=True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b56558-e00c-461a-acc5-7edbe469fff5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Unstructured pdf\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "pdf = pdfplumber.open(r\"data_pdf_2.pdf\")\n",
    "lines = []\n",
    "for page in pdf.pages :\n",
    "    lines.extend(page.extract_text().split(\"\\n\"))\n",
    "header = lines[0].split()\n",
    "data = [line for line in lines[1:] if re.match(r\"\\d{5}\", line.strip())]\n",
    "\n",
    "# Using basic list and str handling\n",
    "def clean_data(string) :\n",
    "    parts = string.split()\n",
    "    return [parts[0], \" \".join(parts[1:4]), parts[4], \" \".join(parts[5:7]), *parts[7:]]\n",
    "\n",
    "# Using regular expression\n",
    "def clean_data_re(string) :\n",
    "    pattern = r'^(\\d+)\\s+(.+?)\\s+([A-Za-z]+)\\s+(Q\\d\\s+\\d{2}|FY\\s+\\d{2})\\s+(\\d{2}-\\d{2}-\\d{4})\\s+(\\d{2}-\\d{2}-\\d{4})\\s+(\\d{2}-\\d{2}-\\d{4})$'\n",
    "    result = re.match(pattern, string)\n",
    "    return result.groups() if result else [np.nan] * 7\n",
    "\n",
    "df = pd.DataFrame(map(clean_data, data), columns = header)\n",
    "# df = pd.DataFrame(map(clean_data_re, data), columns = header)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785ac8a4-e796-4947-89bd-80079d1aae04",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COID CoName\n",
      "\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "10688 Meta Platforms, Inc.\n",
      "\n",
      "Ticker PeriodName PeriodEndDate FirstFillingDate LatestFillingDate\n",
      "\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "META\n",
      "\n",
      "Q107\n",
      "Q2 07\n",
      "Q3 07\n",
      "Q4 07\n",
      "FY 07\n",
      "Q1 08\n",
      "Q2 08\n",
      "Q3 08\n",
      "Q4 08\n",
      "FY 08\n",
      "Q1 09\n",
      "Q2 09\n",
      "Q3 09\n",
      "Q4 09\n",
      "FY 09\n",
      "Q1 10\n",
      "Q2 10\n",
      "Q3 10\n",
      "Q4 10\n",
      "FY 10\n",
      "Qiil\n",
      "Q2 11\n",
      "0311\n",
      "\n",
      "31-03-2007\n",
      "30-06-2007\n",
      "30-09-2007\n",
      "31-12-2007\n",
      "31-12-2007\n",
      "31-03-2008\n",
      "30-06-2008\n",
      "30-09-2008\n",
      "31-12-2008\n",
      "31-12-2008\n",
      "31-03-2009\n",
      "30-06-2009\n",
      "30-09-2009\n",
      "31-12-2009\n",
      "31-12-2009\n",
      "31-03-2010\n",
      "30-06-2010\n",
      "30-09-2010\n",
      "31-12-2010\n",
      "31-12-2010\n",
      "31-03-2011\n",
      "30-06-2011\n",
      "30-09-2011\n",
      "\n",
      "15-04-2007\n",
      "15-07-2007\n",
      "15-10-2007\n",
      "15-01-2008\n",
      "15-01-2008\n",
      "15-04-2008\n",
      "15-07-2008\n",
      "15-10-2008\n",
      "15-01-2009\n",
      "15-01-2009\n",
      "15-04-2009\n",
      "15-07-2009\n",
      "15-10-2009\n",
      "15-01-2010\n",
      "15-01-2010\n",
      "15-04-2010\n",
      "15-07-2010\n",
      "15-10-2010\n",
      "15-01-2011\n",
      "15-01-2011\n",
      "15-04-2011\n",
      "15-07-2011\n",
      "15-10-2011\n",
      "\n",
      "15-04-2007\n",
      "15-07-2007\n",
      "15-10-2007\n",
      "15-01-2008\n",
      "15-01-2008\n",
      "15-04-2008\n",
      "15-07-2008\n",
      "15-10-2008\n",
      "15-01-2009\n",
      "15-01-2009\n",
      "15-04-2009\n",
      "15-07-2009\n",
      "15-10-2009\n",
      "15-01-2010\n",
      "15-01-2010\n",
      "15-04-2010\n",
      "15-07-2010\n",
      "15-10-2010\n",
      "15-01-2011\n",
      "01-02-2013\n",
      "15-04-2011\n",
      "15-07-2011\n",
      "15-10-2011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanned PDF - Teserract\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "# Path to Tesseract executable (adjust this)\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Convert PDF to images\n",
    "pdf_path = \"data_pdf_4.pdf\" #\"data_pdf_4.pdf\" # \"data_pdf_3.pdf\"\n",
    "images = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "\n",
    "# Extract text from each image\n",
    "for page_num, img in enumerate(images):\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67738b-bdab-4467-9da9-4ae5155197fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Scanned PDF - Easy-OCR\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from easyocr import Reader\n",
    "\n",
    "# Convert PDF to images\n",
    "pdf_path = \"data_pdf_4.pdf\" #\"data_pdf_4.pdf\" # \"data_pdf_3.pdf\"\n",
    "images = convert_from_path(pdf_path, dpi=300)\n",
    "reader = Reader(['en'])\n",
    "\n",
    "# Extract text from each image\n",
    "for page_num, img in enumerate(images):\n",
    "    img.save(f'temp.png', 'PNG')\n",
    "    text = reader.readtext(\"temp.png\", detail=0)\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca1f170-266a-4d3d-ad25-5508295698e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\vaide\\.keras-ocr\\craft_mlt_25k.h5\n",
      "Looking for C:\\Users\\vaide\\.keras-ocr\\crnn_kurapan.h5\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "24/24 [==============================] - 83s 3s/step\n",
      "['amows', 'millionss', '103', '203', '502', '1025', '2025', '104', '2024', '5024', 'so2a', '2024', '1025', '2a2se', '302se', 'sazse', '202se', '2026e', 'share', 'ovlal', 'd', 'ereps', 'eer', 'adhortising', 'rotenue', 'sss', 'saog', 'ssoa', 'ss76', 'slgal', 'sot', 'st11', 'sas', 's9ea', 'sipa', 's11s9', 'sixs', 's1255', 'slaos', 'sagna', 'sq175', 's8l', 'sjaa', 'sje0', '87', 'slaa', 's1bd', 's3', 'slass', '825', '525', 'apes', 'reenue', 's369', '565', 'sts0', 'sliss', 'siet', 'siass', 'sszag', 'seulze', 'tatol', 'rotnus', 'st1s', 'shsa', 's954', 'szhs', 'sldss', 'slded', 'sajos', 'sizos', 'sipss', 'slfos', '156', '5267', 'cost', 'of', 'gaap', '5760', 'szl', 'sloag', '5241', 's700', 's761', 's41s', 'slies6', 's769', 's180', 's185', 'siad', 's6la', 'ss69', 'rerenus', 'non', '5156', 'sig', 'scoz', 'sl6m', 'sto7', 'sboo', 'sloss', 's1555', 'slz1', 'slots', 'slizd', '1165', 'sosta', 'groes', 'profit', 'ock', 's931', 'sss', '8l8', 'ales', 'und', 'marletine', 'gaap', 's16s', 's177', 's146', 's7o2', 'staa', 's204', '179', 's186', 's141', 'stco', 's10s', 'sso', 'ss5', 'ss6', '5127', 's857', 'non', 's102', 's103', 'elarch', 'and', 'developtent', 'gaar', 's96', 's106', 'aoy', 's107', 'sllb', 'sice', 's18', 'aa6', '592', 'ses', '87', '89', 's155', 'stes', 'non', 'administratine', 'gaae', '526', 's12', 's17', 's1', 'sn', 's21', 's19', 's20', 's35', 'ss', 's11', 'sj0', '8d', 's8o', 's1s1', 's157', 'getea', 'and', 'non', 'so', 'so', 'so', 'a', 's169', 'so', 'so', 'so', 'viner', 'so', 'so', 'sd', 's164', 's81', 's151', 'sfos', '815', 'si27', 'see1', 'sel', 't1a', 's716', 'sl', '104', 'saso', 's165', 's170', 'siz5', 'seooo', 'ssel', 'dta', 'ecratine', 'and', 'cpsles', 's197', 'sab5s', 'saso', 'ssts', 'operating', 'gaar', 's1ad', 'sle', '858', 'sloi', '616', 'stos', 's20', 'ss', 's910', 'd', 'ssso', 'saizo', 'neote', 'non', 'ouner', 'stock', 'sas', 'ss1', 'sii', 'ses', 's8g', 's95', 'sea', 'sn', 's101', 's7g', 'sgl', 'saa', 'sta', 's91', 'st1a', 's14', 'daed', 'comp', 'gaap', 'opsratns', 'sol', 's181', 'slaoe', 's270', 'soas', 'saad', 's751', 'ses', 'seoc', 'slae', 'soo8', 'sa1n', 'se76', 'sesg', 's7260', 'saa1z', 'neome', 'sts', 'sn', 's2a', 'sia', 'sts', 'sra', 'ss5', 's112', 'interest', 'noomeretenes', 'and', 'dedt', 'settlement', 'ss1', 'sts', 's518', '555', 'sss', '583', 's712', 'orher', 's10', 's15', 'o', '519', 'st', 's8', 's9', 's71', 'so', 'so', 'so', 's4', 'so', 'ncone', 'onponee', 'slog', 'sss', 'sos7', 'pretan', 's86', 's179', 'ssel', '5768', '125', 'ace', 's1st6', 'sols', 'st66', 'sele', 'seag', 'sazco', 'incomc', 'tax', 'uenefepenes', 'sl', 's15', 's24', '3z', 's15', 'sea', '512', '5128', 's15z', 's185', 'sasz', 'sol', 'econe', 'l', '7', 'es', 'gaa', 's1o8', 's56', 'stid', 'sas', 'sst', 's6', 'slga', 'net', 'inoomes', 'soo', 'b', 'slz', 'sss', 'slseo', 's015', 'sool', 'stil', 'ssee', 'non', 'controlling', 'so', 's0', 'ss', 'so', 'so', 'sd', 'sa', 'so', 'so', 'so', 'so', 'so', 'so', 'so', 'o', 'so', 'inerest', 'siog', 'eo', 'a', 'sss9', 'det', 'ape', 'ss', 'sog', 'sit2', 'st', '5166', 'slseo', 'soto', 'soas', 'sosl', 'srel', 'slole', 'sesze', 'ncoine', 'net', 'inuorne', 'alribulable', 'sd', 'so', 'sl', 's1', 's1', 'so', 'so', 'so', 'so', 'so', 'sd', 'so', 'so', 'so', 'to', 'palps', 'lds', 'curlies', 'z', 'sz', 'net', 'attrbutodle', 'arp', 'as16', 'sbuo', 'slo6', 'svis', 's15s', 's55', 'sjid', 'sal', 'ssy', 'sista', 'ss', 'seas', 'suoz', 'stil', 'szgza', 'sjsx', 'income', 'o', 'commmon', 'dscontinuon', 'operatons', '94', '55', '7a', 's106', 's1s', 'soon', 's02z', 'sosp', 'sosl', 's101', 'sd', '5129', 'sld', 'sa', 's1', 's1', 's1o', 'basic', 'eps', 'td', 'sogl', '6o', 'do', 'b9', 'sz', 'oz', 's7', 'od', 's167', 'j0', 'diuted', 'eps', 'soon', 'son', 'soo', 'soag', 'so', '99', 'sog7', 'aeg', 'sl1s', 's118', 'sasa', 's197', 's16g', 'sl9g', 's106', 's754', 'sio1s', 'basie', 'jt', '552', 'j10', '519', 'j1o', 'jals', 'sasd', 'sal6', 'sis', 'ste', 'sst', 'sal', 'ssd', 'seu', 'sex', 'jaz', 'sos', 'fully', 'diluted', '318', 'js0', 'ns', '5t5', 'j0b', '5st', '507', 'sol', 'jad', '518', 'j1o', 'j10', '515', '816', '3t', 'jut', 's10', '5128', 's120', 's12', 's119', 'salg', 's118', 's1o9', 's10o', 's124', 'aa9', 'seo', 'sto', 'sto', 'sto', '150', 's280', 'deprcciation', 's', 'amortization', 'ouner', 's1', 's1', 'o', 'sl', 's2', 's1', '5', 'ss', 's11', 's21', 's200', 'so', 'sd', 'so', 'szo0', 'so', 'adjustod', 'fbitda', 'st', 'sees', 'saise', 's4762', 'sisos', 'ssase', 'seoiz', 'sn16', 'sego', 'stzo', 'sloos', 'slozd', 'saoss', 'sad', 'sloceo', 'ssone', 'fci', 's778', 's121', 's144', 's890', 'sloe7', 's99', 'sa5s', 'sss1', 's6ts', 'stoz', 'st0', 'ssus', 'sl', 'cob', 'ss6s', 'ss7s0', 'sa', '811', 'fcishare', 'sote', 'so6o', 'sosa', 'so', '5s', 'sloo', 'sle', 'sls1', 's15s', 'ses', 'ssso', 'sool', 's2ba', 's79d', '5176', 'sos', 'sese', 'fct', 'conversion', 'byc', 'tos', '95t', '9gh', 'as', 'percent', 'retene', 'or', 'gfoes', 'prolol', 'oete', 'os', 'se', '69', 'tes', 'lloo', '68', 'ld', 'zes', 'zale', 'laeo', '76', '90', 'tsk', 'oloo', 'sszes', 'ssze', 'ss', 'ses', 'slzo', 'cord', 'sales', 'maretine', 'gaar', 'te', '2', '1ss', 'je', '1sn', 's6n', '8', '9s', 'elss', 'az', 'aet', 'gzn', 'son', 'ano', 'non', 'zlzn', 'zle', 'ller', 'sae', 'gaap', '1gs', '105s5', 'gs', 'moetacn', 'and', 'devepetont', 'ses', 'kes', 'pllso', 'kas', 'jos', 's', 'dro', 's', 'ets', '611', '715', 's', 'des', 'otes', 'aess', 'non', 'gerera', 'sominietatie', 'gaap', 'ls', 'le6', 'pznn', 'zos', 'lzx', '2010', '21n', '208', '25n', 'as', '255', '2518', 'zz6', 'ind', 'sin', 'ltes', 'l', 'non', 'ebida', 'adused', 'magn', 'sosn', 'fss', 'toss', 'soa', 'o', 'ssod', 'slse', 'ssto', 'ozo', 'bloro', 'llord', 'bllzo', 'slsa', 'sls', 'oles', 'es', 'ol', 'os', 'lees', 'ax', 'rate', 'gats', '1gis', 'oss', '5i', 'ges', 'jzgs', 'aes', '1s', 'ols', 'bose', '16', 'ox', 'jgos', 'jgos', '1als', 'to', 'dn']\n"
     ]
    }
   ],
   "source": [
    "# Scanned PDF - Keras-OCR\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "import keras_ocr\n",
    "\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "\n",
    "# Convert PDF to images\n",
    "pdf_path = \"data_pdf_5.pdf\" #\"data_pdf_4.pdf\" # \"data_pdf_3.pdf\"\n",
    "images = convert_from_path(pdf_path, dpi=300)\n",
    "\n",
    "# Extract text from each image\n",
    "for page_num, img in enumerate(images):\n",
    "    img.save(f'temp.png', 'PNG')\n",
    "    # Read image\n",
    "    image = keras_ocr.tools.read(\"temp.png\")\n",
    "    prediction_groups = pipeline.recognize([image])\n",
    "\n",
    "    text = [word for word, box in prediction_groups[0]]\n",
    "    print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
